{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.request\n",
    "import time\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from math import ceil\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# open_page Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_sleep = 2 #Sec\n",
    "def open_page(url, time_sleep):\n",
    "    response = requests.get(url, verify=False)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    time.sleep(time_sleep)\n",
    "    return(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ScholarshipDb.Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = \"Scholarship\" \n",
    "program = \"PhD\" \n",
    "query = \"coastal engineering\"\n",
    "query = input()\n",
    "listed = 30\n",
    "page = 1\n",
    "#make url\n",
    "def make_url(category, program, query, listed, page):\n",
    "    query = \"+\".join(query.split())\n",
    "    url = \"https://scholarshipdb.net/scholarships/\"  \n",
    "    if category:\n",
    "        url = url + \"Category-{}/\".format(category)\n",
    "    if program:\n",
    "        url = url + \"Program-{}?\".format(program)\n",
    "    if query:\n",
    "        url = url + \"q={}&\".format(query)\n",
    "    if listed:\n",
    "        url = url + \"listed=Last-{}-days&\".format(str(listed))\n",
    "    if page:\n",
    "        url = url + \"l=&page={}\".format(str(page))\n",
    "    return url\n",
    "url = make_url (category, program, query, listed, page)\n",
    "#find the page\n",
    "soup = open_page(url, time_sleep)\n",
    "pages = list(range(1, ceil(int(soup.select(\"h1.title\")[0].text.split()[0])/10)+1))\n",
    "\n",
    "df = []\n",
    "for page in pages:\n",
    "    url = make_url (category, program, query, listed, page)\n",
    "    soup = open_page(url, time_sleep)\n",
    "    section = soup.select(\"ul.list-unstyled\")[4].select(\"li\")\n",
    "    job_hyperlink = []\n",
    "    for i in range(len(section)):\n",
    "        try:\n",
    "            job_hyperlink.append(\"https://scholarshipdb.net/\" + section[i].find(\"div\").select(\"a\")[-1][\"href\"])\n",
    "        except AttributeError:\n",
    "            pass\n",
    "    df.extend(job_hyperlink)\n",
    "for url in df:\n",
    "    soup = open_page(url, time_sleep)\n",
    "    title = soup.select(\"div.position-details\")[0].find(\"div\").text.lstrip().rstrip()\n",
    "    try:\n",
    "        Deadline = soup.find(\"span\", text=\"Deadline: \").next_sibling\n",
    "    except AttributeError:\n",
    "        Deadline = None\n",
    "    try:\n",
    "        Updated = soup.find(\"span\", text=\"Updated: \").next_sibling.text\n",
    "    except AttributeError:\n",
    "        Updated = None    \n",
    "    try:\n",
    "        Location = soup.find(\"span\", text=\"Location: \").next_sibling.text\n",
    "    except AttributeError:\n",
    "        Location = None\n",
    "    try:    \n",
    "        University = soup.select(\"div.position-details\")[0].find(\"h2\").find(\"a\").text\n",
    "    except AttributeError:\n",
    "        University = None    \n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"job_title\", \"job_location\", \"job_country\", \"job_time\", \"job_introduction\", \"job_hyperlink\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = []\n",
    "for page in pages:\n",
    "    url = make_url (category, program, query, listed, page)\n",
    "    soup = open_page(url)\n",
    "    time.sleep(3)\n",
    "    d = extract_job_defintion_from_result(soup)\n",
    "    d = dict(tuple(list(enumerate(d))))\n",
    "    df2 = pd.DataFrame(d) \n",
    "    df.append(df2)   \n",
    "df = pd.concat(df, ignore_index=True)\n",
    "df.columns = columns\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find A PhD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Funding = \"non-eu-students\" \n",
    "Keyword = input()\n",
    "page = 1\n",
    "def make_url (Keyword, Funding, page):\n",
    "    Keyword = \"+\".join(Keyword.split())\n",
    "    url = \"https://www.findaphd.com/phds/\"   \n",
    "    if Funding:\n",
    "        url = url + \"{}\".format(Funding)\n",
    "    if Keyword:\n",
    "        url = url + \"/?01w0&Keywords={}\".format(Keyword)\n",
    "    if page:\n",
    "        url = url + \"&PG={}\".format(str(page))\n",
    "    return url\n",
    "url = make_url (Keyword, Funding, page)\n",
    "soup = open_page(url, time_sleep)\n",
    "pages = list(range(1, ceil(int(soup.select(\"div.land\")[1].find(\"h4\").text.split()[2])/10)+1))\n",
    "\n",
    "df = []\n",
    "for page in pages:\n",
    "    url = make_url (Keyword, Funding, page)\n",
    "    soup = open_page(url, time_sleep):\n",
    "\n",
    "    links = soup.select(\"a.ctaButton.details\")\n",
    "    job_hyperlink = [\"https://www.findaphd.com\" + link['href'] for link in links]\n",
    "    df.extend(job_hyperlink)\n",
    "for url in df:\n",
    "    soup = open_page(url, time_sleep):\n",
    "    title = soup.select(\"h1\")[0].text\n",
    "    try:\n",
    "        university = soup.select(\"h2.instLink\")[0].text\n",
    "    except:\n",
    "        university = None\n",
    "    try:\n",
    "        department = soup.select(\"h2.deptLink\")[0].text\n",
    "    except:\n",
    "        department = None\n",
    "    try:\n",
    "        country = soup.select(\"h2.ctryLink\")[0].text\n",
    "    except:\n",
    "        country = None        \n",
    "    try:\n",
    "        supervisor = soup.select(\"li.toItem.col-sm-12.col-xs-12\")[0].find(\"a\", attrs={'class':'emailLink'}).text\n",
    "    except:\n",
    "        supervisor = None\n",
    "    try:\n",
    "        body = soup.select(\"div.wrapChildUrls\")[0].text\n",
    "    except:\n",
    "        body = None\n",
    "    print(title)\n",
    "    print(university)\n",
    "    print(department)\n",
    "    print(country)\n",
    "    print(supervisor)\n",
    "print(\"Finished\")\n",
    "print(100*\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EURAXESS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.request\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "Keyword = input()\n",
    "page = 1\n",
    "def make_url (Keyword, page):    \n",
    "    Keyword = \"+\".join(Keyword.split())\n",
    "    url = \"https://euraxess.ec.europa.eu/jobs/search?keywords=\"   \n",
    "    if Keyword:\n",
    "        url = url + \"{}\".format(Keyword)\n",
    "    if page:\n",
    "        url = url + \"&page={}\".format(str(page))\n",
    "    return url\n",
    "url = make_url (Keyword, page)\n",
    "soup = open_page(url, time_sleep):\n",
    "pages = list(range(1, int(ceil(int(soup.select(\"h2.text-center\")[0].text.split()[-1].replace(\"(\", \"\").replace(\")\", \"\")))/10)+1))\n",
    "print(pages)\n",
    "df = []\n",
    "for page in pages:\n",
    "    url = make_url (Keyword, page)\n",
    "    soup = open_page(url, time_sleep):\n",
    "    links = soup.select(\"a.btn.btn-link.action\")[:-1]\n",
    "    job_hyperlink = [\"https://euraxess.ec.europa.eu/\" + link['href'] for link in links]\n",
    "    df.extend(job_hyperlink)\n",
    "    \n",
    "for url in df:\n",
    "    soup = open_page(url, time_sleep):\n",
    "    title= soup.select(\"h1.head-title\")[0].text\n",
    "    institute = soup.select(\"div.col-xs-12.col-sm-7.value.field-company-institute.inline\")[0].text\n",
    "    deadline = soup.select(\"div.col-xs-12.col-sm-7.value.field-application-deadline.inline\")[0].text\n",
    "    country = soup.select(\"div.col-xs-12.col-sm-7.value.field-country.inline\")[0].text\n",
    "    body = soup.select(\"div.longtext.field-body\")[0].text\n",
    "    print(title, deadline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
